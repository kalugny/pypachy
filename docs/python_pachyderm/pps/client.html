<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>python_pachyderm.pps.client API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>python_pachyderm.pps.client</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-

from python_pachyderm._proto.pps import pps_pb2 as proto
from python_pachyderm._proto.pps import pps_pb2_grpc as grpc
from python_pachyderm.util import commit_from, get_address, get_metadata


class PpsClient(object):
    def __init__(self, host=None, port=None, auth_token=None, root_certs=None):
        &#34;&#34;&#34;
        Creates a client to connect to PPS.

        Params:

        * host: The pachd host. Default is &#39;localhost&#39;.
        * port: The port to connect to. Default is 30650.
        * auth_token: The authentication token; used if authentication is
        * enabled on the cluster. Default to `None`.
        * root_certs:  The PEM-encoded root certificates as byte string.
        &#34;&#34;&#34;

        address = get_address(host, port)
        self.metadata = get_metadata(auth_token)
        if root_certs:
            ssl_channel_credentials = grpc.grpc.ssl_channel_credentials
            ssl = ssl_channel_credentials(root_certificates=root_certs)
            self.channel = grpc.grpc.secure_channel(address, ssl)
        else:
            self.channel = grpc.grpc.insecure_channel(address)
        self.stub = grpc.APIStub(self.channel)

    def inspect_job(self, job_id, block_state=None, output_commit=None):
        &#34;&#34;&#34;
        Inspects a job with a given ID. Returns a `JobInfo`.

        Params:

        * job_id: The ID of the job to inspect.
        * block_state: If true, block until the job completes.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        &#34;&#34;&#34;

        output_commit = commit_from(output_commit) if output_commit is not None else None
        req = proto.InspectJobRequest(job=proto.Job(id=job_id), block_state=block_state, output_commit=output_commit)
        return self.stub.InspectJob(req, metadata=self.metadata)

    def list_job(self, pipeline_name=None, input_commit=None, output_commit=None, history=None):
        &#34;&#34;&#34;
        Lists jobs. Yields `JobInfo` objects.

        Params:

        * pipeline_name: An optional string representing a pipeline name to
        filter on.
        * input_commit: An optional list of tuples, strings, or `Commit`
        objects representing input commits to filter on.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        * history: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = proto.Pipeline(name=pipeline_name) if pipeline_name is not None else None

        if isinstance(input_commit, list):
            input_commit = [commit_from(ic) for ic in input_commit]
        elif input_commit is not None:
            input_commit = [commit_from(input_commit)]

        output_commit = commit_from(output_commit) if output_commit is not None else None

        req = proto.ListJobRequest(pipeline=pipeline, input_commit=input_commit,
                                   output_commit=output_commit, history=history)

        return self.stub.ListJobStream(req, metadata=self.metadata)

    def flush_job(self, commits, pipeline_names=None):
        &#34;&#34;&#34;
        Blocks until all of the jobs which have a set of commits as
        provenance have finished. Yields `JobInfo` objects.

        Params:

        * commits: A list of tuples, strings, or `Commit` objects representing
        the commits to flush.
        * pipeline_names: An optional list of strings specifying pipeline
        names. If specified, only jobs within these pipelines will be flushed.
        &#34;&#34;&#34;

        commits = [commit_from(c) for c in commits]
        pipelines = [proto.Pipeline(name=name) for name in pipeline_names] if pipeline_names is not None else None
        req = proto.FlushJobRequest(commits=commits, to_pipelines=pipelines)
        return self.stub.FlushJob(req)

    def delete_job(self, job_id):
        &#34;&#34;&#34;
        Deletes a job by its ID.

        Params:

        * job_id: The ID of the job to delete.
        &#34;&#34;&#34;

        req = proto.DeleteJobRequest(job=proto.Job(id=job_id))
        self.stub.DeleteJob(req, metadata=self.metadata)

    def stop_job(self, job_id):
        &#34;&#34;&#34;
        Stops a job by its ID.

        Params:

        * job_id: The ID of the job to stop.
        &#34;&#34;&#34;

        req = proto.StopJobRequest(job=proto.Job(id=job_id))
        self.stub.StopJob(req, metadata=self.metadata)

    def inspect_datum(self, job_id, datum_id):
        &#34;&#34;&#34;
        Inspects a datum. Returns a `DatumInfo` object.

        Params:

        * job_id: The ID of the job.
        * datum_id: The ID of the datum.
        &#34;&#34;&#34;

        req = proto.InspectDatumRequest(datum=proto.Datum(id=datum_id, job=proto.Job(id=job_id)))
        return self.stub.InspectDatum(req, metadata=self.metadata)

    def list_datum(self, job_id, page_size=None, page=None):
        &#34;&#34;&#34;
        Lists datums. Yields `ListDatumStreamResponse` objects.

        Params:

        * job_id: The ID of the job.
        * page_size: An optional int specifying the size of the page.
        * page: An optional int specifying the page number.
        &#34;&#34;&#34;

        req = proto.ListDatumRequest(job=proto.Job(id=job_id), page_size=page_size, page=page)
        return self.stub.ListDatumStream(req, metadata=self.metadata)

    def restart_datum(self, job_id, data_filters=None):
        &#34;&#34;&#34;
        Restarts a datum.

        Params:

        * job_id: The ID of the job.
        * data_filters: An optional iterable of strings.
        &#34;&#34;&#34;

        req = proto.RestartDatumRequest(job=proto.Job(id=job_id), data_filters=data_filters)
        self.stub.RestartDatum(req, metadata=self.metadata)

    def create_pipeline(self, pipeline_name, transform=None, parallelism_spec=None,
                        hashtree_spec=None, egress=None, update=None, output_branch=None,
                        scale_down_threshold=None, resource_requests=None,
                        resource_limits=None, input=None, description=None, cache_size=None,
                        enable_stats=None, reprocess=None, batch=None, max_queue_size=None,
                        service=None, chunk_spec=None, datum_timeout=None,
                        job_timeout=None, salt=None, standby=None, datum_tries=None,
                        scheduling_spec=None, pod_patch=None):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * pipeline_name: A string representing the pipeline name.
        * transform: An optional `Transform` object.
        * parallelism_spec: An optional `ParallelismSpec` object.
        * hashtree_spec: An optional `HashtreeSpec` object.
        * egress: An optional `Egress` object.
        * update: An optional bool specifying whether this should behave as an
        upsert.
        * output_branch: An optional string representing the branch to output
        results on.
        * scale_down_threshold: An optional protobuf `Duration` object.
        * resource_requests: An optional `ResourceSpec` object.
        * resource_limits: An optional `ResourceSpec` object.
        * input: An optional `Input` object.
        * description: An optional string describing the pipeline.
        * cache_size: An optional string.
        * enable_stats: An optional bool.
        * reprocess: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * batch: An optional bool.
        * max_queue_size: An optional int.
        * service: An optional `Service` object.
        * chunk_spec: An optional `ChunkSpec` object.
        * datum_timeout: An optional protobuf `Duration` object.
        * job_timeout: An optional protobuf `Duration` object.
        * salt: An optional stirng.
        * standby: An optional bool.
        * datum_tries: An optional int.
        * scheduling_spec: An optional `SchedulingSpec` object.
        * pod_patch: An optional string.
        &#34;&#34;&#34;

        req = proto.CreatePipelineRequest(
            pipeline=proto.Pipeline(name=pipeline_name),
            transform=transform, parallelism_spec=parallelism_spec,
            hashtree_spec=hashtree_spec, egress=egress, update=update,
            output_branch=output_branch, scale_down_threshold=scale_down_threshold,
            resource_requests=resource_requests, resource_limits=resource_limits,
            input=input, description=description, cache_size=cache_size,
            enable_stats=enable_stats, reprocess=reprocess, batch=batch,
            max_queue_size=max_queue_size, service=service,
            chunk_spec=chunk_spec, datum_timeout=datum_timeout,
            job_timeout=job_timeout, salt=salt, standby=standby,
            datum_tries=datum_tries, scheduling_spec=scheduling_spec,
            pod_patch=pod_patch
        )
        self.stub.CreatePipeline(req, metadata=self.metadata)

    def inspect_pipeline(self, pipeline_name, history=None):
        &#34;&#34;&#34;
        Inspects a pipeline. Returns a `PipelineInfo` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = proto.Pipeline(name=pipeline_name)

        if history is None:
            req = proto.InspectPipelineRequest(pipeline=pipeline)
            return self.stub.InspectPipeline(req, metadata=self.metadata)
        else:
            # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
            # with a pipeline filter does, so we use that here
            req = proto.ListPipelineRequest(pipeline=pipeline, history=history)
            pipelines = self.stub.ListPipeline(req, metadata=self.metadata).pipeline_info
            assert len(pipelines) &lt;= 1
            return pipelines[0] if len(pipelines) else None

    def list_pipeline(self, history=None):
        &#34;&#34;&#34;
        Lists pipelines. Returns a `PipelineInfos` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;
        req = proto.ListPipelineRequest(history=history)
        return self.stub.ListPipeline(req, metadata=self.metadata)

    def delete_pipeline(self, pipeline_name, force=None):
        &#34;&#34;&#34;
        Deletes a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = proto.DeletePipelineRequest(pipeline=proto.Pipeline(name=pipeline_name), force=force)
        self.stub.DeletePipeline(req, metadata=self.metadata)

    def delete_all_pipelines(self, force=None):
        &#34;&#34;&#34;
        Deletes all pipelines.

        Params:

        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = proto.DeletePipelineRequest(all=True, force=force)
        self.stub.DeletePipeline(req, metadata=self.metadata)

    def start_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Starts a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;

        req = proto.StartPipelineRequest(pipeline=proto.Pipeline(name=pipeline_name))
        self.stub.StartPipeline(req, metadata=self.metadata)

    def stop_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Stops a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;
        req = proto.StopPipelineRequest(pipeline=proto.Pipeline(name=pipeline_name))
        self.stub.StopPipeline(req, metadata=self.metadata)

    def run_pipeline(self, pipeline_name, provenance=None):
        &#34;&#34;&#34;
        Runs a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * provenance: An optional iterable of `CommitProvenance` objects
        representing the pipeline execution provenance.
        &#34;&#34;&#34;
        req = proto.RunPipelineRequest(
            pipeline=proto.Pipeline(name=pipeline_name),
            provenance=provenance,
        )
        self.stub.RunPipeline(req, metadata=self.metadata)

    def delete_all(self):
        &#34;&#34;&#34;
        Deletes everything in pachyderm.
        &#34;&#34;&#34;
        req = proto.google_dot_protobuf_dot_empty__pb2.Empty()
        self.stub.DeleteAll(req, metadata=self.metadata)

    def get_pipeline_logs(self, pipeline_name, data_filters=None, master=None,
                          datum=None, follow=None, tail=None):
        &#34;&#34;&#34;
        Gets logs for a pipeline. Yields `LogMessage` objects.

        Params:

        * pipeline_name: A string representing a pipeline to get
        logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * master: An optional bool.
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = proto.GetLogsRequest(
            pipeline=proto.Pipeline(name=pipeline_name),
            data_filters=data_filters, master=master, datum=datum,
            follow=follow, tail=tail,
        )
        return self.stub.GetLogs(req, metadata=self.metadata)

    def get_job_logs(self, job_id, data_filters=None, datum=None, follow=None,
                     tail=None):
        &#34;&#34;&#34;
        Gets logs for a job. Yields `LogMessage` objects.

        Params:

        * job_id: A string representing a job to get logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = proto.GetLogsRequest(
            job=proto.Job(id=job_id), data_filters=data_filters, datum=datum,
            follow=follow, tail=tail,
        )
        return self.stub.GetLogs(req, metadata=self.metadata)

    def garbage_collect(self):
        &#34;&#34;&#34;
        Runs garbage collection.
        &#34;&#34;&#34;
        return self.stub.GarbageCollect(proto.GarbageCollectRequest(), metadata=self.metadata)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="python_pachyderm.pps.client.PpsClient"><code class="flex name class">
<span>class <span class="ident">PpsClient</span></span>
<span>(</span><span>host=None, port=None, auth_token=None, root_certs=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a client to connect to PPS.</p>
<p>Params:</p>
<ul>
<li>host: The pachd host. Default is 'localhost'.</li>
<li>port: The port to connect to. Default is 30650.</li>
<li>auth_token: The authentication token; used if authentication is</li>
<li>enabled on the cluster. Default to <code>None</code>.</li>
<li>root_certs:
The PEM-encoded root certificates as byte string.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class PpsClient(object):
    def __init__(self, host=None, port=None, auth_token=None, root_certs=None):
        &#34;&#34;&#34;
        Creates a client to connect to PPS.

        Params:

        * host: The pachd host. Default is &#39;localhost&#39;.
        * port: The port to connect to. Default is 30650.
        * auth_token: The authentication token; used if authentication is
        * enabled on the cluster. Default to `None`.
        * root_certs:  The PEM-encoded root certificates as byte string.
        &#34;&#34;&#34;

        address = get_address(host, port)
        self.metadata = get_metadata(auth_token)
        if root_certs:
            ssl_channel_credentials = grpc.grpc.ssl_channel_credentials
            ssl = ssl_channel_credentials(root_certificates=root_certs)
            self.channel = grpc.grpc.secure_channel(address, ssl)
        else:
            self.channel = grpc.grpc.insecure_channel(address)
        self.stub = grpc.APIStub(self.channel)

    def inspect_job(self, job_id, block_state=None, output_commit=None):
        &#34;&#34;&#34;
        Inspects a job with a given ID. Returns a `JobInfo`.

        Params:

        * job_id: The ID of the job to inspect.
        * block_state: If true, block until the job completes.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        &#34;&#34;&#34;

        output_commit = commit_from(output_commit) if output_commit is not None else None
        req = proto.InspectJobRequest(job=proto.Job(id=job_id), block_state=block_state, output_commit=output_commit)
        return self.stub.InspectJob(req, metadata=self.metadata)

    def list_job(self, pipeline_name=None, input_commit=None, output_commit=None, history=None):
        &#34;&#34;&#34;
        Lists jobs. Yields `JobInfo` objects.

        Params:

        * pipeline_name: An optional string representing a pipeline name to
        filter on.
        * input_commit: An optional list of tuples, strings, or `Commit`
        objects representing input commits to filter on.
        * output_commit: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        * history: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = proto.Pipeline(name=pipeline_name) if pipeline_name is not None else None

        if isinstance(input_commit, list):
            input_commit = [commit_from(ic) for ic in input_commit]
        elif input_commit is not None:
            input_commit = [commit_from(input_commit)]

        output_commit = commit_from(output_commit) if output_commit is not None else None

        req = proto.ListJobRequest(pipeline=pipeline, input_commit=input_commit,
                                   output_commit=output_commit, history=history)

        return self.stub.ListJobStream(req, metadata=self.metadata)

    def flush_job(self, commits, pipeline_names=None):
        &#34;&#34;&#34;
        Blocks until all of the jobs which have a set of commits as
        provenance have finished. Yields `JobInfo` objects.

        Params:

        * commits: A list of tuples, strings, or `Commit` objects representing
        the commits to flush.
        * pipeline_names: An optional list of strings specifying pipeline
        names. If specified, only jobs within these pipelines will be flushed.
        &#34;&#34;&#34;

        commits = [commit_from(c) for c in commits]
        pipelines = [proto.Pipeline(name=name) for name in pipeline_names] if pipeline_names is not None else None
        req = proto.FlushJobRequest(commits=commits, to_pipelines=pipelines)
        return self.stub.FlushJob(req)

    def delete_job(self, job_id):
        &#34;&#34;&#34;
        Deletes a job by its ID.

        Params:

        * job_id: The ID of the job to delete.
        &#34;&#34;&#34;

        req = proto.DeleteJobRequest(job=proto.Job(id=job_id))
        self.stub.DeleteJob(req, metadata=self.metadata)

    def stop_job(self, job_id):
        &#34;&#34;&#34;
        Stops a job by its ID.

        Params:

        * job_id: The ID of the job to stop.
        &#34;&#34;&#34;

        req = proto.StopJobRequest(job=proto.Job(id=job_id))
        self.stub.StopJob(req, metadata=self.metadata)

    def inspect_datum(self, job_id, datum_id):
        &#34;&#34;&#34;
        Inspects a datum. Returns a `DatumInfo` object.

        Params:

        * job_id: The ID of the job.
        * datum_id: The ID of the datum.
        &#34;&#34;&#34;

        req = proto.InspectDatumRequest(datum=proto.Datum(id=datum_id, job=proto.Job(id=job_id)))
        return self.stub.InspectDatum(req, metadata=self.metadata)

    def list_datum(self, job_id, page_size=None, page=None):
        &#34;&#34;&#34;
        Lists datums. Yields `ListDatumStreamResponse` objects.

        Params:

        * job_id: The ID of the job.
        * page_size: An optional int specifying the size of the page.
        * page: An optional int specifying the page number.
        &#34;&#34;&#34;

        req = proto.ListDatumRequest(job=proto.Job(id=job_id), page_size=page_size, page=page)
        return self.stub.ListDatumStream(req, metadata=self.metadata)

    def restart_datum(self, job_id, data_filters=None):
        &#34;&#34;&#34;
        Restarts a datum.

        Params:

        * job_id: The ID of the job.
        * data_filters: An optional iterable of strings.
        &#34;&#34;&#34;

        req = proto.RestartDatumRequest(job=proto.Job(id=job_id), data_filters=data_filters)
        self.stub.RestartDatum(req, metadata=self.metadata)

    def create_pipeline(self, pipeline_name, transform=None, parallelism_spec=None,
                        hashtree_spec=None, egress=None, update=None, output_branch=None,
                        scale_down_threshold=None, resource_requests=None,
                        resource_limits=None, input=None, description=None, cache_size=None,
                        enable_stats=None, reprocess=None, batch=None, max_queue_size=None,
                        service=None, chunk_spec=None, datum_timeout=None,
                        job_timeout=None, salt=None, standby=None, datum_tries=None,
                        scheduling_spec=None, pod_patch=None):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * pipeline_name: A string representing the pipeline name.
        * transform: An optional `Transform` object.
        * parallelism_spec: An optional `ParallelismSpec` object.
        * hashtree_spec: An optional `HashtreeSpec` object.
        * egress: An optional `Egress` object.
        * update: An optional bool specifying whether this should behave as an
        upsert.
        * output_branch: An optional string representing the branch to output
        results on.
        * scale_down_threshold: An optional protobuf `Duration` object.
        * resource_requests: An optional `ResourceSpec` object.
        * resource_limits: An optional `ResourceSpec` object.
        * input: An optional `Input` object.
        * description: An optional string describing the pipeline.
        * cache_size: An optional string.
        * enable_stats: An optional bool.
        * reprocess: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * batch: An optional bool.
        * max_queue_size: An optional int.
        * service: An optional `Service` object.
        * chunk_spec: An optional `ChunkSpec` object.
        * datum_timeout: An optional protobuf `Duration` object.
        * job_timeout: An optional protobuf `Duration` object.
        * salt: An optional stirng.
        * standby: An optional bool.
        * datum_tries: An optional int.
        * scheduling_spec: An optional `SchedulingSpec` object.
        * pod_patch: An optional string.
        &#34;&#34;&#34;

        req = proto.CreatePipelineRequest(
            pipeline=proto.Pipeline(name=pipeline_name),
            transform=transform, parallelism_spec=parallelism_spec,
            hashtree_spec=hashtree_spec, egress=egress, update=update,
            output_branch=output_branch, scale_down_threshold=scale_down_threshold,
            resource_requests=resource_requests, resource_limits=resource_limits,
            input=input, description=description, cache_size=cache_size,
            enable_stats=enable_stats, reprocess=reprocess, batch=batch,
            max_queue_size=max_queue_size, service=service,
            chunk_spec=chunk_spec, datum_timeout=datum_timeout,
            job_timeout=job_timeout, salt=salt, standby=standby,
            datum_tries=datum_tries, scheduling_spec=scheduling_spec,
            pod_patch=pod_patch
        )
        self.stub.CreatePipeline(req, metadata=self.metadata)

    def inspect_pipeline(self, pipeline_name, history=None):
        &#34;&#34;&#34;
        Inspects a pipeline. Returns a `PipelineInfo` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;

        pipeline = proto.Pipeline(name=pipeline_name)

        if history is None:
            req = proto.InspectPipelineRequest(pipeline=pipeline)
            return self.stub.InspectPipeline(req, metadata=self.metadata)
        else:
            # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
            # with a pipeline filter does, so we use that here
            req = proto.ListPipelineRequest(pipeline=pipeline, history=history)
            pipelines = self.stub.ListPipeline(req, metadata=self.metadata).pipeline_info
            assert len(pipelines) &lt;= 1
            return pipelines[0] if len(pipelines) else None

    def list_pipeline(self, history=None):
        &#34;&#34;&#34;
        Lists pipelines. Returns a `PipelineInfos` object.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * history: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;
        req = proto.ListPipelineRequest(history=history)
        return self.stub.ListPipeline(req, metadata=self.metadata)

    def delete_pipeline(self, pipeline_name, force=None):
        &#34;&#34;&#34;
        Deletes a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = proto.DeletePipelineRequest(pipeline=proto.Pipeline(name=pipeline_name), force=force)
        self.stub.DeletePipeline(req, metadata=self.metadata)

    def delete_all_pipelines(self, force=None):
        &#34;&#34;&#34;
        Deletes all pipelines.

        Params:

        * force: Whether to force delete.
        &#34;&#34;&#34;

        req = proto.DeletePipelineRequest(all=True, force=force)
        self.stub.DeletePipeline(req, metadata=self.metadata)

    def start_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Starts a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;

        req = proto.StartPipelineRequest(pipeline=proto.Pipeline(name=pipeline_name))
        self.stub.StartPipeline(req, metadata=self.metadata)

    def stop_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Stops a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        &#34;&#34;&#34;
        req = proto.StopPipelineRequest(pipeline=proto.Pipeline(name=pipeline_name))
        self.stub.StopPipeline(req, metadata=self.metadata)

    def run_pipeline(self, pipeline_name, provenance=None):
        &#34;&#34;&#34;
        Runs a pipeline.

        Params:

        * pipeline_name: A string representing the pipeline name.
        * provenance: An optional iterable of `CommitProvenance` objects
        representing the pipeline execution provenance.
        &#34;&#34;&#34;
        req = proto.RunPipelineRequest(
            pipeline=proto.Pipeline(name=pipeline_name),
            provenance=provenance,
        )
        self.stub.RunPipeline(req, metadata=self.metadata)

    def delete_all(self):
        &#34;&#34;&#34;
        Deletes everything in pachyderm.
        &#34;&#34;&#34;
        req = proto.google_dot_protobuf_dot_empty__pb2.Empty()
        self.stub.DeleteAll(req, metadata=self.metadata)

    def get_pipeline_logs(self, pipeline_name, data_filters=None, master=None,
                          datum=None, follow=None, tail=None):
        &#34;&#34;&#34;
        Gets logs for a pipeline. Yields `LogMessage` objects.

        Params:

        * pipeline_name: A string representing a pipeline to get
        logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * master: An optional bool.
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = proto.GetLogsRequest(
            pipeline=proto.Pipeline(name=pipeline_name),
            data_filters=data_filters, master=master, datum=datum,
            follow=follow, tail=tail,
        )
        return self.stub.GetLogs(req, metadata=self.metadata)

    def get_job_logs(self, job_id, data_filters=None, datum=None, follow=None,
                     tail=None):
        &#34;&#34;&#34;
        Gets logs for a job. Yields `LogMessage` objects.

        Params:

        * job_id: A string representing a job to get logs of.
        * data_filters: An optional iterable of strings specifying the names
        of input files from which we want processing logs. This may contain
        multiple files, to query pipelines that contain multiple inputs. Each
        filter may be an absolute path of a file within a pps repo, or it may
        be a hash for that file (to search for files at specific versions.)
        * datum: An optional `Datum` object.
        * follow: An optional bool specifying whether logs should continue to
        stream forever.
        * tail: An optional int. If nonzero, the number of lines from the end
        of the logs to return.  Note: tail applies per container, so you will
        get tail * &lt;number of pods&gt; total lines back.
        &#34;&#34;&#34;

        req = proto.GetLogsRequest(
            job=proto.Job(id=job_id), data_filters=data_filters, datum=datum,
            follow=follow, tail=tail,
        )
        return self.stub.GetLogs(req, metadata=self.metadata)

    def garbage_collect(self):
        &#34;&#34;&#34;
        Runs garbage collection.
        &#34;&#34;&#34;
        return self.stub.GarbageCollect(proto.GarbageCollectRequest(), metadata=self.metadata)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="python_pachyderm.pps.client.PpsClient.create_pipeline"><code class="name flex">
<span>def <span class="ident">create_pipeline</span></span>(<span>self, pipeline_name, transform=None, parallelism_spec=None, hashtree_spec=None, egress=None, update=None, output_branch=None, scale_down_threshold=None, resource_requests=None, resource_limits=None, input=None, description=None, cache_size=None, enable_stats=None, reprocess=None, batch=None, max_queue_size=None, service=None, chunk_spec=None, datum_timeout=None, job_timeout=None, salt=None, standby=None, datum_tries=None, scheduling_spec=None, pod_patch=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a pipeline. For more info, please refer to the pipeline spec
document:
<a href="http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html">http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html</a></p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>transform: An optional <code>Transform</code> object.</li>
<li>parallelism_spec: An optional <code>ParallelismSpec</code> object.</li>
<li>hashtree_spec: An optional <code>HashtreeSpec</code> object.</li>
<li>egress: An optional <code>Egress</code> object.</li>
<li>update: An optional bool specifying whether this should behave as an
upsert.</li>
<li>output_branch: An optional string representing the branch to output
results on.</li>
<li>scale_down_threshold: An optional protobuf <code>Duration</code> object.</li>
<li>resource_requests: An optional <code>ResourceSpec</code> object.</li>
<li>resource_limits: An optional <code>ResourceSpec</code> object.</li>
<li>input: An optional <code>Input</code> object.</li>
<li>description: An optional string describing the pipeline.</li>
<li>cache_size: An optional string.</li>
<li>enable_stats: An optional bool.</li>
<li>reprocess: An optional bool. If true, pachyderm forces the pipeline
to reprocess all datums. It only has meaning if <code>update</code> is <code>True</code>.</li>
<li>batch: An optional bool.</li>
<li>max_queue_size: An optional int.</li>
<li>service: An optional <code>Service</code> object.</li>
<li>chunk_spec: An optional <code>ChunkSpec</code> object.</li>
<li>datum_timeout: An optional protobuf <code>Duration</code> object.</li>
<li>job_timeout: An optional protobuf <code>Duration</code> object.</li>
<li>salt: An optional stirng.</li>
<li>standby: An optional bool.</li>
<li>datum_tries: An optional int.</li>
<li>scheduling_spec: An optional <code>SchedulingSpec</code> object.</li>
<li>pod_patch: An optional string.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def create_pipeline(self, pipeline_name, transform=None, parallelism_spec=None,
                    hashtree_spec=None, egress=None, update=None, output_branch=None,
                    scale_down_threshold=None, resource_requests=None,
                    resource_limits=None, input=None, description=None, cache_size=None,
                    enable_stats=None, reprocess=None, batch=None, max_queue_size=None,
                    service=None, chunk_spec=None, datum_timeout=None,
                    job_timeout=None, salt=None, standby=None, datum_tries=None,
                    scheduling_spec=None, pod_patch=None):
    &#34;&#34;&#34;
    Creates a pipeline. For more info, please refer to the pipeline spec
    document:
    http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

    Params:

    * pipeline_name: A string representing the pipeline name.
    * transform: An optional `Transform` object.
    * parallelism_spec: An optional `ParallelismSpec` object.
    * hashtree_spec: An optional `HashtreeSpec` object.
    * egress: An optional `Egress` object.
    * update: An optional bool specifying whether this should behave as an
    upsert.
    * output_branch: An optional string representing the branch to output
    results on.
    * scale_down_threshold: An optional protobuf `Duration` object.
    * resource_requests: An optional `ResourceSpec` object.
    * resource_limits: An optional `ResourceSpec` object.
    * input: An optional `Input` object.
    * description: An optional string describing the pipeline.
    * cache_size: An optional string.
    * enable_stats: An optional bool.
    * reprocess: An optional bool. If true, pachyderm forces the pipeline
    to reprocess all datums. It only has meaning if `update` is `True`.
    * batch: An optional bool.
    * max_queue_size: An optional int.
    * service: An optional `Service` object.
    * chunk_spec: An optional `ChunkSpec` object.
    * datum_timeout: An optional protobuf `Duration` object.
    * job_timeout: An optional protobuf `Duration` object.
    * salt: An optional stirng.
    * standby: An optional bool.
    * datum_tries: An optional int.
    * scheduling_spec: An optional `SchedulingSpec` object.
    * pod_patch: An optional string.
    &#34;&#34;&#34;

    req = proto.CreatePipelineRequest(
        pipeline=proto.Pipeline(name=pipeline_name),
        transform=transform, parallelism_spec=parallelism_spec,
        hashtree_spec=hashtree_spec, egress=egress, update=update,
        output_branch=output_branch, scale_down_threshold=scale_down_threshold,
        resource_requests=resource_requests, resource_limits=resource_limits,
        input=input, description=description, cache_size=cache_size,
        enable_stats=enable_stats, reprocess=reprocess, batch=batch,
        max_queue_size=max_queue_size, service=service,
        chunk_spec=chunk_spec, datum_timeout=datum_timeout,
        job_timeout=job_timeout, salt=salt, standby=standby,
        datum_tries=datum_tries, scheduling_spec=scheduling_spec,
        pod_patch=pod_patch
    )
    self.stub.CreatePipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.delete_all"><code class="name flex">
<span>def <span class="ident">delete_all</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes everything in pachyderm.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_all(self):
    &#34;&#34;&#34;
    Deletes everything in pachyderm.
    &#34;&#34;&#34;
    req = proto.google_dot_protobuf_dot_empty__pb2.Empty()
    self.stub.DeleteAll(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.delete_all_pipelines"><code class="name flex">
<span>def <span class="ident">delete_all_pipelines</span></span>(<span>self, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes all pipelines.</p>
<p>Params:</p>
<ul>
<li>force: Whether to force delete.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_all_pipelines(self, force=None):
    &#34;&#34;&#34;
    Deletes all pipelines.

    Params:

    * force: Whether to force delete.
    &#34;&#34;&#34;

    req = proto.DeletePipelineRequest(all=True, force=force)
    self.stub.DeletePipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.delete_job"><code class="name flex">
<span>def <span class="ident">delete_job</span></span>(<span>self, job_id)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a job by its ID.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job to delete.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_job(self, job_id):
    &#34;&#34;&#34;
    Deletes a job by its ID.

    Params:

    * job_id: The ID of the job to delete.
    &#34;&#34;&#34;

    req = proto.DeleteJobRequest(job=proto.Job(id=job_id))
    self.stub.DeleteJob(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.delete_pipeline"><code class="name flex">
<span>def <span class="ident">delete_pipeline</span></span>(<span>self, pipeline_name, force=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Deletes a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>force: Whether to force delete.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def delete_pipeline(self, pipeline_name, force=None):
    &#34;&#34;&#34;
    Deletes a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * force: Whether to force delete.
    &#34;&#34;&#34;

    req = proto.DeletePipelineRequest(pipeline=proto.Pipeline(name=pipeline_name), force=force)
    self.stub.DeletePipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.flush_job"><code class="name flex">
<span>def <span class="ident">flush_job</span></span>(<span>self, commits, pipeline_names=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Blocks until all of the jobs which have a set of commits as
provenance have finished. Yields <code>JobInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>commits: A list of tuples, strings, or <code>Commit</code> objects representing
the commits to flush.</li>
<li>pipeline_names: An optional list of strings specifying pipeline
names. If specified, only jobs within these pipelines will be flushed.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def flush_job(self, commits, pipeline_names=None):
    &#34;&#34;&#34;
    Blocks until all of the jobs which have a set of commits as
    provenance have finished. Yields `JobInfo` objects.

    Params:

    * commits: A list of tuples, strings, or `Commit` objects representing
    the commits to flush.
    * pipeline_names: An optional list of strings specifying pipeline
    names. If specified, only jobs within these pipelines will be flushed.
    &#34;&#34;&#34;

    commits = [commit_from(c) for c in commits]
    pipelines = [proto.Pipeline(name=name) for name in pipeline_names] if pipeline_names is not None else None
    req = proto.FlushJobRequest(commits=commits, to_pipelines=pipelines)
    return self.stub.FlushJob(req)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.garbage_collect"><code class="name flex">
<span>def <span class="ident">garbage_collect</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Runs garbage collection.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def garbage_collect(self):
    &#34;&#34;&#34;
    Runs garbage collection.
    &#34;&#34;&#34;
    return self.stub.GarbageCollect(proto.GarbageCollectRequest(), metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.get_job_logs"><code class="name flex">
<span>def <span class="ident">get_job_logs</span></span>(<span>self, job_id, data_filters=None, datum=None, follow=None, tail=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets logs for a job. Yields <code>LogMessage</code> objects.</p>
<p>Params:</p>
<ul>
<li>job_id: A string representing a job to get logs of.</li>
<li>data_filters: An optional iterable of strings specifying the names
of input files from which we want processing logs. This may contain
multiple files, to query pipelines that contain multiple inputs. Each
filter may be an absolute path of a file within a pps repo, or it may
be a hash for that file (to search for files at specific versions.)</li>
<li>datum: An optional <code>Datum</code> object.</li>
<li>follow: An optional bool specifying whether logs should continue to
stream forever.</li>
<li>tail: An optional int. If nonzero, the number of lines from the end
of the logs to return.
Note: tail applies per container, so you will
get tail * <number of pods> total lines back.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_job_logs(self, job_id, data_filters=None, datum=None, follow=None,
                 tail=None):
    &#34;&#34;&#34;
    Gets logs for a job. Yields `LogMessage` objects.

    Params:

    * job_id: A string representing a job to get logs of.
    * data_filters: An optional iterable of strings specifying the names
    of input files from which we want processing logs. This may contain
    multiple files, to query pipelines that contain multiple inputs. Each
    filter may be an absolute path of a file within a pps repo, or it may
    be a hash for that file (to search for files at specific versions.)
    * datum: An optional `Datum` object.
    * follow: An optional bool specifying whether logs should continue to
    stream forever.
    * tail: An optional int. If nonzero, the number of lines from the end
    of the logs to return.  Note: tail applies per container, so you will
    get tail * &lt;number of pods&gt; total lines back.
    &#34;&#34;&#34;

    req = proto.GetLogsRequest(
        job=proto.Job(id=job_id), data_filters=data_filters, datum=datum,
        follow=follow, tail=tail,
    )
    return self.stub.GetLogs(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.get_pipeline_logs"><code class="name flex">
<span>def <span class="ident">get_pipeline_logs</span></span>(<span>self, pipeline_name, data_filters=None, master=None, datum=None, follow=None, tail=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets logs for a pipeline. Yields <code>LogMessage</code> objects.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing a pipeline to get
logs of.</li>
<li>data_filters: An optional iterable of strings specifying the names
of input files from which we want processing logs. This may contain
multiple files, to query pipelines that contain multiple inputs. Each
filter may be an absolute path of a file within a pps repo, or it may
be a hash for that file (to search for files at specific versions.)</li>
<li>master: An optional bool.</li>
<li>datum: An optional <code>Datum</code> object.</li>
<li>follow: An optional bool specifying whether logs should continue to
stream forever.</li>
<li>tail: An optional int. If nonzero, the number of lines from the end
of the logs to return.
Note: tail applies per container, so you will
get tail * <number of pods> total lines back.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_pipeline_logs(self, pipeline_name, data_filters=None, master=None,
                      datum=None, follow=None, tail=None):
    &#34;&#34;&#34;
    Gets logs for a pipeline. Yields `LogMessage` objects.

    Params:

    * pipeline_name: A string representing a pipeline to get
    logs of.
    * data_filters: An optional iterable of strings specifying the names
    of input files from which we want processing logs. This may contain
    multiple files, to query pipelines that contain multiple inputs. Each
    filter may be an absolute path of a file within a pps repo, or it may
    be a hash for that file (to search for files at specific versions.)
    * master: An optional bool.
    * datum: An optional `Datum` object.
    * follow: An optional bool specifying whether logs should continue to
    stream forever.
    * tail: An optional int. If nonzero, the number of lines from the end
    of the logs to return.  Note: tail applies per container, so you will
    get tail * &lt;number of pods&gt; total lines back.
    &#34;&#34;&#34;

    req = proto.GetLogsRequest(
        pipeline=proto.Pipeline(name=pipeline_name),
        data_filters=data_filters, master=master, datum=datum,
        follow=follow, tail=tail,
    )
    return self.stub.GetLogs(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.inspect_datum"><code class="name flex">
<span>def <span class="ident">inspect_datum</span></span>(<span>self, job_id, datum_id)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a datum. Returns a <code>DatumInfo</code> object.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job.</li>
<li>datum_id: The ID of the datum.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_datum(self, job_id, datum_id):
    &#34;&#34;&#34;
    Inspects a datum. Returns a `DatumInfo` object.

    Params:

    * job_id: The ID of the job.
    * datum_id: The ID of the datum.
    &#34;&#34;&#34;

    req = proto.InspectDatumRequest(datum=proto.Datum(id=datum_id, job=proto.Job(id=job_id)))
    return self.stub.InspectDatum(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.inspect_job"><code class="name flex">
<span>def <span class="ident">inspect_job</span></span>(<span>self, job_id, block_state=None, output_commit=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a job with a given ID. Returns a <code>JobInfo</code>.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job to inspect.</li>
<li>block_state: If true, block until the job completes.</li>
<li>output_commit: An optional tuple, string, or <code>Commit</code> object
representing an output commit to filter on.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_job(self, job_id, block_state=None, output_commit=None):
    &#34;&#34;&#34;
    Inspects a job with a given ID. Returns a `JobInfo`.

    Params:

    * job_id: The ID of the job to inspect.
    * block_state: If true, block until the job completes.
    * output_commit: An optional tuple, string, or `Commit` object
    representing an output commit to filter on.
    &#34;&#34;&#34;

    output_commit = commit_from(output_commit) if output_commit is not None else None
    req = proto.InspectJobRequest(job=proto.Job(id=job_id), block_state=block_state, output_commit=output_commit)
    return self.stub.InspectJob(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.inspect_pipeline"><code class="name flex">
<span>def <span class="ident">inspect_pipeline</span></span>(<span>self, pipeline_name, history=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Inspects a pipeline. Returns a <code>PipelineInfo</code> object.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def inspect_pipeline(self, pipeline_name, history=None):
    &#34;&#34;&#34;
    Inspects a pipeline. Returns a `PipelineInfo` object.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * history: An optional int that indicates to return jobs from
    historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;

    pipeline = proto.Pipeline(name=pipeline_name)

    if history is None:
        req = proto.InspectPipelineRequest(pipeline=pipeline)
        return self.stub.InspectPipeline(req, metadata=self.metadata)
    else:
        # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
        # with a pipeline filter does, so we use that here
        req = proto.ListPipelineRequest(pipeline=pipeline, history=history)
        pipelines = self.stub.ListPipeline(req, metadata=self.metadata).pipeline_info
        assert len(pipelines) &lt;= 1
        return pipelines[0] if len(pipelines) else None</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.list_datum"><code class="name flex">
<span>def <span class="ident">list_datum</span></span>(<span>self, job_id, page_size=None, page=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists datums. Yields <code>ListDatumStreamResponse</code> objects.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job.</li>
<li>page_size: An optional int specifying the size of the page.</li>
<li>page: An optional int specifying the page number.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_datum(self, job_id, page_size=None, page=None):
    &#34;&#34;&#34;
    Lists datums. Yields `ListDatumStreamResponse` objects.

    Params:

    * job_id: The ID of the job.
    * page_size: An optional int specifying the size of the page.
    * page: An optional int specifying the page number.
    &#34;&#34;&#34;

    req = proto.ListDatumRequest(job=proto.Job(id=job_id), page_size=page_size, page=page)
    return self.stub.ListDatumStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.list_job"><code class="name flex">
<span>def <span class="ident">list_job</span></span>(<span>self, pipeline_name=None, input_commit=None, output_commit=None, history=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists jobs. Yields <code>JobInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: An optional string representing a pipeline name to
filter on.</li>
<li>input_commit: An optional list of tuples, strings, or <code>Commit</code>
objects representing input commits to filter on.</li>
<li>output_commit: An optional tuple, string, or <code>Commit</code> object
representing an output commit to filter on.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_job(self, pipeline_name=None, input_commit=None, output_commit=None, history=None):
    &#34;&#34;&#34;
    Lists jobs. Yields `JobInfo` objects.

    Params:

    * pipeline_name: An optional string representing a pipeline name to
    filter on.
    * input_commit: An optional list of tuples, strings, or `Commit`
    objects representing input commits to filter on.
    * output_commit: An optional tuple, string, or `Commit` object
    representing an output commit to filter on.
    * history: An optional int that indicates to return jobs from
      historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;

    pipeline = proto.Pipeline(name=pipeline_name) if pipeline_name is not None else None

    if isinstance(input_commit, list):
        input_commit = [commit_from(ic) for ic in input_commit]
    elif input_commit is not None:
        input_commit = [commit_from(input_commit)]

    output_commit = commit_from(output_commit) if output_commit is not None else None

    req = proto.ListJobRequest(pipeline=pipeline, input_commit=input_commit,
                               output_commit=output_commit, history=history)

    return self.stub.ListJobStream(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.list_pipeline"><code class="name flex">
<span>def <span class="ident">list_pipeline</span></span>(<span>self, history=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Lists pipelines. Returns a <code>PipelineInfos</code> object.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>history: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def list_pipeline(self, history=None):
    &#34;&#34;&#34;
    Lists pipelines. Returns a `PipelineInfos` object.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * history: An optional int that indicates to return jobs from
    historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;
    req = proto.ListPipelineRequest(history=history)
    return self.stub.ListPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.restart_datum"><code class="name flex">
<span>def <span class="ident">restart_datum</span></span>(<span>self, job_id, data_filters=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Restarts a datum.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job.</li>
<li>data_filters: An optional iterable of strings.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def restart_datum(self, job_id, data_filters=None):
    &#34;&#34;&#34;
    Restarts a datum.

    Params:

    * job_id: The ID of the job.
    * data_filters: An optional iterable of strings.
    &#34;&#34;&#34;

    req = proto.RestartDatumRequest(job=proto.Job(id=job_id), data_filters=data_filters)
    self.stub.RestartDatum(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.run_pipeline"><code class="name flex">
<span>def <span class="ident">run_pipeline</span></span>(<span>self, pipeline_name, provenance=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Runs a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
<li>provenance: An optional iterable of <code>CommitProvenance</code> objects
representing the pipeline execution provenance.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def run_pipeline(self, pipeline_name, provenance=None):
    &#34;&#34;&#34;
    Runs a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    * provenance: An optional iterable of `CommitProvenance` objects
    representing the pipeline execution provenance.
    &#34;&#34;&#34;
    req = proto.RunPipelineRequest(
        pipeline=proto.Pipeline(name=pipeline_name),
        provenance=provenance,
    )
    self.stub.RunPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.start_pipeline"><code class="name flex">
<span>def <span class="ident">start_pipeline</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Starts a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def start_pipeline(self, pipeline_name):
    &#34;&#34;&#34;
    Starts a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    &#34;&#34;&#34;

    req = proto.StartPipelineRequest(pipeline=proto.Pipeline(name=pipeline_name))
    self.stub.StartPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.stop_job"><code class="name flex">
<span>def <span class="ident">stop_job</span></span>(<span>self, job_id)</span>
</code></dt>
<dd>
<section class="desc"><p>Stops a job by its ID.</p>
<p>Params:</p>
<ul>
<li>job_id: The ID of the job to stop.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def stop_job(self, job_id):
    &#34;&#34;&#34;
    Stops a job by its ID.

    Params:

    * job_id: The ID of the job to stop.
    &#34;&#34;&#34;

    req = proto.StopJobRequest(job=proto.Job(id=job_id))
    self.stub.StopJob(req, metadata=self.metadata)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.pps.client.PpsClient.stop_pipeline"><code class="name flex">
<span>def <span class="ident">stop_pipeline</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Stops a pipeline.</p>
<p>Params:</p>
<ul>
<li>pipeline_name: A string representing the pipeline name.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def stop_pipeline(self, pipeline_name):
    &#34;&#34;&#34;
    Stops a pipeline.

    Params:

    * pipeline_name: A string representing the pipeline name.
    &#34;&#34;&#34;
    req = proto.StopPipelineRequest(pipeline=proto.Pipeline(name=pipeline_name))
    self.stub.StopPipeline(req, metadata=self.metadata)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="python_pachyderm.pps" href="index.html">python_pachyderm.pps</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="python_pachyderm.pps.client.PpsClient" href="#python_pachyderm.pps.client.PpsClient">PpsClient</a></code></h4>
<ul class="">
<li><code><a title="python_pachyderm.pps.client.PpsClient.create_pipeline" href="#python_pachyderm.pps.client.PpsClient.create_pipeline">create_pipeline</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.delete_all" href="#python_pachyderm.pps.client.PpsClient.delete_all">delete_all</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.delete_all_pipelines" href="#python_pachyderm.pps.client.PpsClient.delete_all_pipelines">delete_all_pipelines</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.delete_job" href="#python_pachyderm.pps.client.PpsClient.delete_job">delete_job</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.delete_pipeline" href="#python_pachyderm.pps.client.PpsClient.delete_pipeline">delete_pipeline</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.flush_job" href="#python_pachyderm.pps.client.PpsClient.flush_job">flush_job</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.garbage_collect" href="#python_pachyderm.pps.client.PpsClient.garbage_collect">garbage_collect</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.get_job_logs" href="#python_pachyderm.pps.client.PpsClient.get_job_logs">get_job_logs</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.get_pipeline_logs" href="#python_pachyderm.pps.client.PpsClient.get_pipeline_logs">get_pipeline_logs</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.inspect_datum" href="#python_pachyderm.pps.client.PpsClient.inspect_datum">inspect_datum</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.inspect_job" href="#python_pachyderm.pps.client.PpsClient.inspect_job">inspect_job</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.inspect_pipeline" href="#python_pachyderm.pps.client.PpsClient.inspect_pipeline">inspect_pipeline</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.list_datum" href="#python_pachyderm.pps.client.PpsClient.list_datum">list_datum</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.list_job" href="#python_pachyderm.pps.client.PpsClient.list_job">list_job</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.list_pipeline" href="#python_pachyderm.pps.client.PpsClient.list_pipeline">list_pipeline</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.restart_datum" href="#python_pachyderm.pps.client.PpsClient.restart_datum">restart_datum</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.run_pipeline" href="#python_pachyderm.pps.client.PpsClient.run_pipeline">run_pipeline</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.start_pipeline" href="#python_pachyderm.pps.client.PpsClient.start_pipeline">start_pipeline</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.stop_job" href="#python_pachyderm.pps.client.PpsClient.stop_job">stop_job</a></code></li>
<li><code><a title="python_pachyderm.pps.client.PpsClient.stop_pipeline" href="#python_pachyderm.pps.client.PpsClient.stop_pipeline">stop_pipeline</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>